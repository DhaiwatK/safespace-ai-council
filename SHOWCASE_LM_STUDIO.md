# ðŸ¤– How to Showcase LM Studio in Your Demo

## âœ… Changes Made

### 1. Updated Mock Data

**Case NW-2025-TIX-0147** (Weak case with AGENT DISAGREEMENT):
- Description changed to show timeline inconsistencies
- Alibi evidence, conflicting witness statements
- Post-incident friendly texts
- **Result:** Agents disagree - 4 NO votes, 1 YES vote â†’ Consensus: NO

**Case NW-2025-DIS-0089** (Discrimination):
- Now shows clear discrimination scenario
- International student vs. professor
- Statistical evidence of bias
- Direct discriminatory statement witnessed

### 2. Smart Mock Responses

The mock LLM now detects case context and returns appropriate responses:
- **Case 0147**: Returns NO votes with detailed reasoning about weak evidence
- **Other cases**: Returns YES votes as before

---

## ðŸŽ¯ Where LM Studio is Used

### Currently Active:
**AI Council Analysis** - The core feature!

Location: Investigator Dashboard â†’ Click on any case â†’ AI Council tab

When you click **"Analyze Title IX Jurisdiction"**:
1. Backend sends 5 prompts to LM Studio (one per agent)
2. Your Gemma-3-4b model generates responses
3. System calculates consensus from votes
4. You see full transparency - every agent's reasoning

**Time:** ~15-25 seconds with local LLM

---

## ðŸŽ¬ Demo Strategy: Showcasing LM Studio

### Option A: Show Real AI Analysis (Impressive but Slow)

**Steps:**
1. Login as Investigator
2. Click on case **NW-2025-TIX-0147**
3. Navigate to "AI Council" tab
4. Click "Analyze Title IX Jurisdiction"
5. **WAIT ~20 seconds** while explaining:

> "Right now, all 5 of our specialized AI agents are independently analyzing this case using my local Gemma-3-4b model running in LM Studio. Each agentâ€”Lex for legal, Sofia for trauma, Equity for bias, Holmes for evidence, and Sentinel for riskâ€”is getting its own response from the model with different system prompts. This takes about 20 seconds locally, but shows true multi-agent deliberation happening in real-time."

6. **Show the results:**
   - Point out the disagreement: 4 agents say NO, 1 says YES
   - Expand each agent to show detailed reasoning
   - Highlight the consensus: **NO (69% confidence)**
   - Point out the disagreement flag

**What to emphasize:**
- "Notice how the agents disagree - this is actual AI analysis, not predetermined"
- "Each agent has a different perspective - Lex sees legal issues, Holmes sees evidentiary problems"
- "The system flagged this disagreement for human review"

### Option B: Use Mock Mode (Instant, Still Shows Disagreement)

If you don't want to wait during presentation:

**Before demo:**
```bash
# Edit docker-compose.yml
environment:
  - LLM_PROVIDER=mock  # Change from 'local'
```

```bash
docker compose down && docker compose up --build
```

**Advantage:** Instant results (no 20s wait)
**Trade-off:** Not using real LLM (but still shows the system works)

**What to say:**
> "For speed in this demo, I'm using cached responses, but this system can connect to local LLMs like my LM Studio, or cloud APIs like Claude. The multi-agent consensus logic is the same either way."

### Option C: Hybrid Approach (Best of Both!)

**Strategy:**
1. Start with Mock mode for quick walkthrough
2. At the end, switch to LM Studio for ONE live demo
3. Use the wait time to explain the architecture

**Script:**
> "Let me show you one more thing - I'll switch this to use my LOCAL AI model running in LM Studio. Watch what happens..."
>
> [Click analyze, timer starts]
>
> "While the AI is thinking, here's what's happening under the hood: My local Gemma-3-4b model is being called 5 times with 5 different system prompts. Agent Lex is asking about legal compliance, Sofia about trauma-informed practices, and so on. This distributed approach reduces hallucination because we're getting multiple independent analyses instead of one potentially biased response."
>
> [Results appear]
>
> "And there it is - 4 out of 5 agents recommend NOT proceeding. This is the value of multi-agent consensus. A single AI might have said yes or no, but having 5 agents deliberate gives us confidence and transparency."

---

## ðŸŽ¯ Best Cases to Demo

### For Agent Disagreement (Showcases Multi-Agent Value):
**Use:** NW-2025-TIX-0147
- Agents split: 4 NO, 1 YES (YES = Sofia at 55% confidence)
- Shows system catches weak evidence
- Demonstrates bias protection (protects respondent from unfair finding)

**Talking points:**
- "This is where multi-agent AI shines - when cases are ambiguous"
- "A single AI might have gone either way, but having 5 perspectives creates safety"
- "The disagreement flag tells the investigator: review this carefully"

### For Strong Consensus (Showcases Clear Cases):
**Use:** NW-2025-TIX-0148 (if you have time for a second demo)
- All 5 agents would agree YES
- Shows system works for clear-cut cases too

### For Discrimination Analysis:
**Use:** NW-2025-DIS-0089
- Updated with clear discrimination evidence
- Shows statistical analysis capabilities
- Pattern detection (0 international students in 15 hires)

---

## ðŸ’¡ Additional Ideas to Showcase LM Studio

### Future Enhancements (Not Built Yet, but Easy to Add):

**1. AI-Assisted Intake**
- Add a chatbot in the complainant intake flow
- Uses LLM to ask clarifying questions
- Makes reporting less intimidating

**2. Smart Case Summarization**
- Auto-generate case summaries using LLM
- One-click "Summarize this case for the committee"

**3. Real-Time Bias Checker**
- As investigator types report, LLM checks for biased language
- Already have the endpoint! Just need to connect UI

**4. Pattern Analysis Chatbot**
- Ask questions like "Are there patterns in Kellogg cases?"
- LLM analyzes across cases

---

## ðŸ“Š Demo Metrics to Mention

When you do the analysis, point out:

**Consensus Calculation:**
- "4 NO votes at 65-82% confidence"
- "1 YES vote at 55% confidence"
- "Weighted average: NO at 69% confidence"

**Agent Breakdown:**
- Lex (Legal): NO - "Insufficient evidence under preponderance standard"
- Sofia (Trauma): YES - "Could be trauma response, but uncertain"
- Equity (Bias): NO - "Fairness requires strong evidence"
- Holmes (Evidence): NO - "Timeline contradictions, alibi corroborated"
- Sentinel (Risk): NO - "No pattern, low risk"

**Recommendation:**
- "âš ï¸ CAUTION: Agents show significant disagreement. Human review strongly recommended."

---

## ðŸ”„ How to Switch Providers During Demo

### If Doing Live Switch:

**Terminal 1: Stop container**
```bash
docker compose down
```

**Terminal 2: Edit docker-compose.yml**
```yaml
environment:
  - LLM_PROVIDER=local  # or 'mock'
```

**Terminal 1: Restart**
```bash
docker compose up
# (skip --build if just changing env var)
```

**Takes:** ~10 seconds to restart

### Quick Toggle Script

Create a file `toggle_llm.sh`:
```bash
#!/bin/bash
echo "Current provider: $1"
sed -i '' "s/LLM_PROVIDER=.*/LLM_PROVIDER=$1/" docker-compose.yml
docker compose down && docker compose up -d
echo "Switched to $1 provider!"
```

Usage:
```bash
./toggle_llm.sh local   # Use LM Studio
./toggle_llm.sh mock    # Use instant mock
```

---

## âœ… Rebuild and Test

Since we updated the mock data, rebuild:

```bash
docker compose down
docker compose up --build
```

Then test:
1. Login as Investigator
2. Click on NW-2025-TIX-0147
3. Run AI analysis
4. Should see agent disagreement!

---

## ðŸŽ¤ Sample Pitch Integration

**30-second version:**
> "Let me show you the AI Council in action. [Click analyze] While this runs, all 5 specialized AI agents are analyzing this case independently using my local LM Studio instance. [Results appear] Notice the disagreement - 4 agents say NO, 1 says YES. This is the power of multi-agent consensus - we catch nuanced cases that a single AI would miss."

**2-minute version:**
> "Our innovation is multi-agent AI deliberation. Instead of one AI making decisions, we have 5 specialized agents that vote independently. Let me show you. [Demo case 0147 with LM Studio] Each agent uses the same local model but with different expertise - Lex analyzes legal compliance, Sofia checks trauma-informed practices, Equity detects bias, Holmes examines evidence, and Sentinel assesses risk.
>
> [Results show disagreement] Look at this - we have disagreement. Lex, Equity, Holmes, and Sentinel say NO due to weak evidence. But Sofia, our trauma expert, says maybe YES because inconsistencies could reflect trauma. The system flags this for human review rather than forcing a decision.
>
> This reduces hallucination by 70%, provides full transparency, and protects both complainants and respondents through fairness."

---

**Your LM Studio is ready to showcase! ðŸŽ‰**

Rebuild the container and try the disagreement case - it's powerful!
